{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minhanh15/MinhAnh_HDS_Blog/blob/main/Week-07-Exercise02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDL7xEHbLpKc"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1DXUVHxd4t15mfuqMgMCLnsP4jWVI5EWz)\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "© Copyright The University of New South Wales - CRICOS 00098G\n",
        "\n",
        "**Author**: Oscar Perez-Concha: o.perezconcha@unsw.edu.au\n",
        "\n",
        "**Contributors/Co-authors**: Marta Fredes-Torres and Zhisheng (Sandy) Sa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Week 7: Artificial Neural Networks / Deep Learning\n",
        "# Exercise 02: Add regularization to a neural network by using Dropout\n",
        "\n",
        "\n",
        "# 1. Introduction\n",
        "\n",
        "In this exercise, we will regularize the deep neural network we built in the previous exercise.\n",
        "\n",
        "\"Dropout works by probabilistically removing, or “dropping out,” inputs to a layer, which may be input variables in the data sample or activations from a previous layer.\"\n",
        "\n",
        "Read: [How to reduce overfitting with dropout regularizationn in keras](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)\n",
        "\n",
        "In particular, pay attention to the section: MLP Dropout Regularization.\n",
        "\n",
        "\n",
        "## 1.1. Aims of the Exercise:\n",
        "\n",
        "1. This is an introduction to Artificial Neural Networks / Deep Learning with regularisation.\n",
        "2. We will use Keras.\n",
        "\n",
        "\n",
        "It aligns with all of our course learning outcomes:\n",
        "\n",
        "1.\tDistinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
        "2.\tDesign machine learning tasks for Health Data Science scenarios.\n",
        "\n",
        "\n",
        "## 1.2. Jupyter Notebook Instructions\n",
        "1. Read the content of each cell.\n",
        "2. Where necessary, follow the instructions that are written in each cell.\n",
        "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
        "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
        "\n",
        "## 1.3. Tips\n",
        "1. Run all the cells in sequence (one at a time), using the \"Run\" button.\n",
        "2. To edit this notebook, just double-click in each cell. Choose between \"Code\" cell or text \"Markdown\" cell in the combo-box above.\n",
        "3. If you want to save your notebook, please go File->Save a copy on Drive/GitHub.\n",
        "4. To clean the content of all cells and re-start Notebook, please go to Edit->Clear all outputs then Runtime->Restart runtime\n",
        "\n",
        "Follow the instructions given and if you have any questions, please use the **Comments section** in **Open Learning**."
      ],
      "metadata": {
        "id": "upiAS75QLqpb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIWDjs2TLpKn"
      },
      "source": [
        "# 2. Docstring:\n",
        "\n",
        "Create a docstring with the variables and constants that you will use in this exercise (data dictionary) and the purpose of your program. It is expected that you choose informative variable names and document your program (both docstrings and comments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmwN4IdMLpKo"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "\n",
        "#####################################################################################################################\n",
        "\n",
        "(double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEVFkM70LpKq"
      },
      "source": [
        "# 3. Load the Malaria Cell Images Data set\n",
        "\n",
        "For more information about this dataset, please see [Tensorflow Malaria](https://www.tensorflow.org/datasets/catalog/malaria) and [NLM - Malaria Data](https://lhncbc.nlm.nih.gov/LHC-research/LHC-projects/image-processing/malaria-datasheet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egAQioyHLpKr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "# FYI: This notebook was created with Python version 3.6.5\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RkVmzL-LpKt"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADoY68FXLpKu"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# We do not need to run this cell if you are not running this notebook in Google Colab\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive # import drive from Gogle colab\n",
        "    root = '/content/drive'     # default location for the drive\n",
        "    # print(root)                 # print content of ROOT (Optional)\n",
        "    drive.mount(root)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEWdW81vLpKw"
      },
      "source": [
        "If you are running this notebook in Google Colab, you must define your project paths. In this case, define your `project_path`. Otherwise, the model output will be lost after you close the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev9AOXg4LpKx"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n",
        "    # T\n",
        "    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'week05'\n",
        "\n",
        "    # OPTIONAL - set working directory according to your google drive project path\n",
        "    # import os\n",
        "    # Change directory to the location defined in project_path\n",
        "    # os.chdir(project_path)\n",
        "else:\n",
        "    project_path = Path()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAIo3lrLpKz"
      },
      "source": [
        "Let's retrieve the arrays \"data\" and \"labels\" that we stored in exercise 1 using binary files in NumPy .npy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q7zPmO0LpK0"
      },
      "outputs": [],
      "source": [
        "data_path = Path(project_path) / 'malaria_img.npz'\n",
        "with np.load(data_path) as img:\n",
        "    data = img['data']\n",
        "    labels = img['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI8Qurw9LpK1"
      },
      "outputs": [],
      "source": [
        "# Sanity Check\n",
        "print('Cells : {} | labels : {}'.format(data.shape , labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dNGjVLoLpK2"
      },
      "source": [
        "**Splitting the dataset into the Training set and Test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbNreuGtLpK2"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 0, stratify = labels)\n",
        "\n",
        "# Sanity Check\n",
        "print(f'SHAPE OF TRAINING IMAGE DATA : {X_train.shape}')\n",
        "print(f'SHAPE OF TESTING IMAGE DATA : {X_test.shape}')\n",
        "print(f'SHAPE OF TRAINING LABELS : {y_train.shape}')\n",
        "print(f'SHAPE OF TESTING LABELS : {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chAuNh-6LpK3"
      },
      "source": [
        "# 4. Our first ANN using Keras + Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKlxKNntLpK4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Flatten\n",
        "from keras import backend as K\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr9PPS36LpK4"
      },
      "source": [
        "Please note that the dropout regularisation is a technique to stop over-fitting of the data. The dropout regularisation probabilistically drops a set of nodes during each training epoch. When a node is not available during training, the remaining nodes have to \"learn\" how to deal with the data being processed. This reduces the ability of the nodes to specialise, and hence prevents over-fitting. The final model does not drop any node; the corresponding weights reflect that learning process of having dropped nodes every now and then during the training stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d6fwaILpK5"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksliQQd9LpK5"
      },
      "source": [
        "### <font color='brown'> Question 1: We create a function `create_model` that will create the skeleton of the neural network. Add dropout regularisation to the first hidden layer.  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-v1u0g7LpK6"
      },
      "outputs": [],
      "source": [
        "def create_model(DropoutL1):\n",
        "    # Initiate model\n",
        "    model = keras.Sequential()\n",
        "    # Add first hidden layer and input\n",
        "    model.add(Dense(16, activation='relu', input_dim = X_train.shape[1], kernel_initializer='uniform'))\n",
        "\n",
        "    # Apply dropout to outputs of hidden layer1\n",
        "    model.add(Dropout(DropoutL1))\n",
        "\n",
        "\n",
        "    # Add second hidden layer\n",
        "    model.add(Dense(16, activation = 'relu', kernel_initializer='uniform'))\n",
        "    # Add output layer\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRHpbEcvLpK6"
      },
      "source": [
        "We need a package that acts as  a wrapper between keras and sklearn. [More information](https://medium.com/@am.benatmane/keras-hyperparameter-tuning-using-sklearn-pipelines-grid-search-with-cross-validation-ccfc74b0ce9f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooGEvuq6LpK7"
      },
      "outputs": [],
      "source": [
        "# Additional packages\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUjGhEmPLpK7"
      },
      "source": [
        "We build our model and we call it \"ann\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHfDt5ztLpK8"
      },
      "outputs": [],
      "source": [
        "ann = KerasClassifier(build_fn=create_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMxSrm7lLpK8"
      },
      "source": [
        "Pipeline and parameter grid for GridSearchCV:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOs3FtI4LpK8"
      },
      "source": [
        "1. Scale the data.\n",
        "\n",
        "2. GridsearchCV for the correct value of Dropout: The correct solution will be to use a pipeline that scales the data inside GridSearchCV. We know that when we use GridsearchCV, the only option for scaling the data is a pipeline. This is because we carry out cross-validation within the traninig set. That is, we divide the data into training and validation k times (as many as k-CV) in order to tune the hyperparameters. Therefore, we scale the data in different folds every time. A pipeline does it automatically for us.\n",
        "\n",
        "3. The GridSearchCV will take a long time. Feel free to choose less hyper-parameters, except for the dropout (0.2 and 0.4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhytkto7LpK9"
      },
      "outputs": [],
      "source": [
        "# Pipeline\n",
        "from sklearn.pipeline  import *\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "Scaler = StandardScaler()\n",
        "# Scaler2 = MinMaxScaler() # Creates values between zero and 1 vs mean of 0 and SD of 1, does not retain scaled variance\n",
        "\n",
        "steps=[('Transform', Scaler), ('ANN', ann)]\n",
        "\n",
        "ann_pipeln = Pipeline(steps)\n",
        "\n",
        "# Create grid for testing density options. It will take a long time to run. Feel free to use less hyper-parameters.\n",
        "param_grid = {'ANN__epochs':[100, 200],\n",
        "              'ANN__batch_size': [200, 500],\n",
        "              'ANN__DropoutL1':[0.2, 0.4]}\n",
        "\n",
        "# Establish GridSearchCV\n",
        "grid_search = GridSearchCV(estimator = ann_pipeln, param_grid = param_grid, cv=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hfhkBJ5TLpK9"
      },
      "outputs": [],
      "source": [
        "# Apply Gridsearch, fit the model with the best params identified in the GridSearch\n",
        "grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teDZx4XSLpK9"
      },
      "source": [
        "### <font color='brown'> Question 2: Calculate accuracy, confusion matrix and all the metrics included in classification_report function. </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka97f54sLpK-"
      },
      "outputs": [],
      "source": [
        "# Write Python code here\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict labels for the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhyR_zSmLpK-"
      },
      "source": [
        "### <font color='brown'> Question 3: Write your conclusions about the performance and potential use of this classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lznHIOtJLpK-"
      },
      "source": [
        "<b> Write answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_LIwCKvLpK_"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OIMflvILpK_"
      },
      "source": [
        "© 2022 Copyright The University of New South Wales - CRICOS 00098G"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Week-05-Exercise02.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}